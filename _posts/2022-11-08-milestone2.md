---
layout: post
title: Milestone 2
---


## Step 1: Data Acquisition




## Step 2: Feature Engineering I

i. Histogram of shot angle and distances.

![alt-text-1](/figures/shot_dist.png "title-1") ![alt-text-2](/figures/angle_hist.png "title-2")

2D histogram of shot angle and distances.

![image](/figures/2d.png "Title")

ii.
![alt-text-1](/figures/score_rate_dist.png "title-1") ![alt-text-2](/figures/score_rate_angle.png "title-2")

In the above graphs, we can see what we wuld expect from these charts. Regarding distance, we see that the goal rate is highest closest t the goal and decreases somewhat linearly as distance incereases. It's quite obvious that this would be true as we would expect more goals the closer a player's shot is. In contrast, the goal rate of the angle appears normally distributed with higher tails on both extremes. This also would be what is expected. Obviously, the best place to shoot, is right infront of the goalie. However, during passing plays, angles that are more extereme are likely to result in goals too.

iii.

![alt-text-1](/figures/hist_empty.png "title-1") ![alt-text-2](/figures/emptynet_dist.png "title-2")

The empty net shot/goal histogram is what we expect. Players take less shots very close since there is no goalie, resulting in the vast majority of shots being taken a bit further out but still in the zone. However, we see that the distribution of goals remains somewhat constant. The majority of goals are being scored in the offensive half, but there are a lot being scored very far out. This can most likely be attributed errors in the the coordinates.


## Step 3: Baseline Models

## Step 4: Feature Engineering II 

## Step 5: Advanced Models

## Step 6: Give it your best shot!

i.

We approached this task wanting get a better idea at how to handle unbalanced data. The goal to shot ratio is about 1:19, thus our data set was quite unbalanced. We can see clearly in the previous couple tasks that our models, and specifically they way we trained our models, yielded pretty much useless results as all X_i points were getting classified as non-goals. From IMBLearn, there is a function called SMOTE which uses the principles from K-Nearest Neighbours to synthetically generate more statistically similar training data-points from the minority class. We decided to use this approach along with better feature selection methods to optimize a simple Neural Network and an XGBoost model which is best suited for this type of problem.

Our first model was just a baseline XGBoost model with all relevant/possible features [here](https://www.comet.com/nhl-07/try-best/67ec3c39233a491f8caa17d9ad304ddd). The second model was an XGBoost using a feature selection method that selected the best set of features for performance ranked on the feature's importance coefficients [here](https://www.comet.com/nhl-07/try-best/369a7d23e3124c6f91dc9b25246f390f). Our third model was an XGBoost using the SMOTE to generate more goal data points [here](https://www.comet.com/nhl-07/try-best/31ad70d81a024e30bebbf7c6532f68d4). Our fourth also used XGBoost but used the feature selection and SMOTE approach combined [here](https://www.comet.com/nhl-07/try-best/8ff12dd37c8c482680d2afdc98996c43). We then moved on to neural networks. The fifth model was just a 3-layer network [here](https://www.comet.com/nhl-07/try-best/e93c967b981c4be289fbb39f3b0d71e5) and the sixth was the same network with the SMOTE approach [here](https://www.comet.com/nhl-07/try-best/125ec88b646b426bbb257b34acf54a31).

We also wanted to focus on different metrics. For unbalanced datasets, we need to use metrics such as f1-score, sensitivity, specificity, positive predictive value and negative preductive value. These metrics will help use determine which classes are being predicted accurately. 

### Metrics

| Model     | Sensitivity | Specificity      | F1-Score | PPV     | NPV |
| ----------- | ----------- | ----------- | ----------- | ----------- | ----------- |
| XGB Base    | 0.15 | 0.99    | 0.60 | 0.69     | 0.92 |
| XBG + Freature Selection     | 0.06 | 1.00     | 0.54 | 0.82     | 0.91 |
| XBG + SMOTE  | 0.15| 0.99      | 0.60 | 0.68     | 0.92 |
|  XBG + SMOTE +FS  | 0.05 | 1.00     | 0.52| 0.64     | 0.91 |
| NN Base    | 0.56| 0.20     | 0.22 | 0.07    | 0.82 |
| NN +SMOTE   | 0.34 | 0.34    | 0.28 | 0.05     | 0.83 |



![alt-text-1](/figures/realiability.png "title-19") ![alt-text-2](/figures/roc.png "title-21")
![alt-text-1](/figures/cumalitive.png "title-12") ![alt-text-2](/figures/goal_rate.png "title-23")

What we got from testing these models is that the binary classification is extremely difficult. We're faced with the extreme tradeoff of PPV ad sensitivity. That is to say, we can classify many more goals correctly but at the extreme expense of precision and vice-versa. After analysis of results, we determined that XGB: SMOTE is the best predictor with a relatively high f1-score. 



## Step 7: Evaluate on test set


Try your best:
Regular Season
| Model     | Sensitivity | Specificity      | F1-Score | PPV     | NPV |
| ----------- | ----------- | ----------- | ----------- | ----------- | ----------- |
| XGB SMOTE   | 0.19 | 0.97    | 0.60 | 0.40     | 0.92 |

Playoff Season
| Model     | Sensitivity | Specificity      | F1-Score | PPV     | NPV |
| ----------- | ----------- | ----------- | ----------- | ----------- | ----------- |
| XGB SMOTE   | 0.17 | 0.99    | 0.61 | 0.46     | 0.92 |

![alt-text-1](/figures/rely.png "title-19") ![alt-text-2](/figures/rc_test.png "title-21")
![alt-text-1](/figures/goal_test.png "title-12") ![alt-text-2](/figures/cum_test.png "title-23")






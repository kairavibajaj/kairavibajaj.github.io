---
layout: post
title: Milestone 2
---

## Step 2: Feature Engineering I

i. Histogram of shot angle and distances.

![alt-text-1](/figures/shot_dist.png "title-1") ![alt-text-2](/figures/angle_hist.png "title-2")

2D histogram of shot angle and distances.

![image](/figures/2d.png "Title")

ii.
![alt-text-1](/figures/score_rate_dist.png "title-1") ![alt-text-2](/figures/score_rate_angle.png "title-2")

In the above graphs, we can see what we wuld expect from these charts. Regarding distance, we see that the goal rate is highest closest t the goal and decreases somewhat linearly as distance incereases. It's quite obvious that this would be true as we would expect more goals the closer a player's shot is. In contrast, the goal rate of the angle appears normally distributed with higher tails on both extremes. This also would be what is expected. Obviously, the best place to shoot, is right infront of the goalie. However, during passing plays, angles that are more extereme are likely to result in goals too.

iii.

![alt-text-1](/figures/hist_empty.png "title-1") ![alt-text-2](/figures/emptynet_dist.png "title-2")

The empty net shot/goal histogram is what we expect. Players take less shots very close since there is no goalie, resulting in the vast majority of shots being taken a bit further out but still in the zone. However, we see that the distribution of goals remains somewhat constant. The majority of goals are being scored in the offensive half, but there are a lot being scored very far out. This can most likely be attributed errors in the the coordinates.

After filtering the dataframe, we have found two data points that are inconsistent. That is because their shot distance was more than 150 which is not normal as it should be less than 100 even in extreme cases which shows that the x-coordinate and y-coordinate for these events were wrong.


| Game ID     | Team             | Event    | Shooter     | X-coord | Y-coord | Distance |
| ----------- | -----------      | ---------| ----------- | --------| --------|----------|
| 2019021001  | Los Angeles Kings| Goal     | Dustin Brown| 89      | 0       | 166.07   |
| 2019021001  | Los Angeles Kings| Goal     | Adrian Kempe| 89      | 0       | 154.73   |



## Step 3: Baseline Models

After training the Logistic Regression model on the distance feature and evaluating on the validation set, we obtained the following results.

{% highlight js %}
Accuracy: 0.9051013734466972
Confusion matrix:
[[55356     0]
 [ 5804     0]]
Classification report:
              precision    recall  f1-score   support

           0       0.91      1.00      0.95     55356
           1       0.00      0.00      0.00      5804

    accuracy                           0.91     61160
   macro avg       0.45      0.50      0.48     61160
weighted avg       0.82      0.91      0.86     61160

{% endhighlight %}

It is clear that the data is unbalanced with the ratio of goals to shots being very low and hence, the model does not actually predict anything. Although, we are getting an accuracy of 90%, it is not an appropriate measure to evaluate the performance of our model considering the unbalanced dataset. The model just predicts everything as a shot and still we end up getting an accuracy of 90%. This is what the potential issue is that the data is not uniformly distributed between the two classes, one class is in majority as compared to the other. Hence, the misleading results.

Following graphs indicate the metrics used to evaluate the three logistic regression models trained on distance feature, angle feature and both angle and distance features respectively and a random classifier. 

Also, refer to the following links to comet experiments of each of the logistic regression models.

Logistic Regression - Distance feature [Comet_entry](https://www.comet.com/nhl-07/baseline-models/e9f966f6346b4ebe935baf399b2dcd78?experiment-tab=chart&showOutliers=true&smoothing=0&transformY=smoothing&xAxis=wall)

Logistic Regression - Angle feature [Comet_entry](https://www.comet.com/nhl-07/baseline-models/6f598b457c6c4642a925b3908e5d4bed?experiment-tab=chart&showOutliers=true&smoothing=0&transformY=smoothing&xAxis=wall)

Logistic Regression - Distance and Angle features [Comet_entry](https://www.comet.com/nhl-07/baseline-models/70a99d5326de420e911449311ecee0d4?experiment-tab=chart&showOutliers=true&smoothing=0&transformY=smoothing&xAxis=wall)
![alt-text-1](/figures/logreg figures/roc.png "title-1") ![alt-text-1](/figures/logreg figures/goal_rate.png "title-1")
![alt-text-1](/figures/logreg figures/cumulative_prop.png "title-1")
![alt-text-1](/figures/logreg figures/reliability.png "title-1")





## Step 4: Feature Engineering II 
## Added Features
- __prev_event_type__   :   The type of last event
- __prev_xcoord__       :   X coordinate of previous event
- __prev_ycoord__       :   Y coordinate of previous event
- __prev_dist__         :   The distance between current event and last event
- __time_from_prev__    :   The time (in seconds) between the previous event and current event
- __rebound__           :   True if last event was also a shot
- __change_in_angle__   :   The change in shot angle between previous event and current event (0.0 if the previous event is not a shot, i.e. rebound is False)
- __speed__             :   distance from the previous event divided by the time since previous event
- __friendly_to_shooter__   :   Number of non-goalie skaters on the ice who are in the same team as the shooter
- __unfriendly_to_shooter__ :    Number of non-goalie skaters on the ice who are on the opposing team of the shooter
- __time_from_powerplay__   :   Time (in seconds) since the powerplay started.

The final dataframe can be found at this [comet_entry](https://www.comet.com/nhl-07/feature-engineering-2/c168e39042574527b2b567d31d3484b0?experiment-tab=assets)

## Step 5: Advanced Models

Now that we have generated a few more new features, let's try to predict using them. For this task we are going to use XGBoost classifiers.
![alt-text-1](/figures/advanced models/fig1.png "title-1")

XGBoost
XGBoost is the leading model for working with standard tabular data (the type of data you store in Pandas DataFrames, as opposed to more exotic types of data like images and videos). XGBoost models dominate many Kaggle competitions.

To reach peak accuracy, XGBoost models require more knowledge and model tuning than techniques like Random Forest. After this tutorial, you'll be able to

Follow the full modeling workflow with XGBoost
Fine-tune XGBoost models for optimal performance
XGBoost is an implementation of the Gradient Boosted Decision Trees algorithm (scikit-learn has another version of this algorithm, but XGBoost has some technical advantages.) What are Gradient Boosted Decision Trees? We'll walk through a diagram.

Preparing the data:
Before training the classifier, We have to prepare our data.

Training-validation setup:
So we will use the 2015/16 season all the way to the 2019/20 season (inclusive) for training and validation. We have randomly splitted it in 80% - 20% split for training and validation respectively.

Also, there is a small portion of data with missing values, so instead of imputing them, we have removed the rows containing missing values. This is the safest way as it won't affect the source distribution.

Apart from basic data cleaning operations, there are some requirements for XGBoost to achieve top performance. Mainly:

* Numeric features should be scaled
* Categorical features should be encoded

Scaling the data:

We have scaled the data using standardization. We have used sklearn’s StandardScalar object. It scales columns using
![alt-text-1](/figures/advanced models/fig2.png "title-1")

There are no columns in Q1 that need encoding.

Q1:
Link to [comet_entry](https://www.comet.com/nhl-07/advanced-models-1/ca1dfc4f42d44ae69554019638bb8f9d?experiment-tab=chart&showOutliers=true&smoothing=0&transformY=smoothing&xAxis=step)

Approach 1: Using Distance Feature only
Link to separate [comet_entry](https://www.comet.com/nhl-07/advanced-models-1/d6753ae511414261a71ea36b83d191ae?experiment-tab=chart&showOutliers=true&smoothing=0&transformY=smoothing&xAxis=step)

Approach 2: Using Angle Feature only
Link to separate [comet_entry](https://www.comet.com/nhl-07/advanced-models-1/a9b970e6d1fb4066b8260b3f652e9f2e?experiment-tab=chart&showOutliers=true&smoothing=0&transformY=smoothing&xAxis=step)

Approach 3: Using Distance and Angle Features
Link to separate [comet_entry](https://www.comet.com/nhl-07/advanced-models-1/ca1dfc4f42d44ae69554019638bb8f9d?experiment-tab=chart&showOutliers=true&smoothing=0&transformY=smoothing&xAxis=step)

Comparison of Approach 3 with Logistic Regression Baseline 
![alt-text-1](/figures/advanced models/xgb-angle+distance/roc.png "title")

![alt-text-1](/figures/advanced models/xgb-angle+distance/goal_rate.png "title")

![alt-text-1](/figures/advanced models/xgb-angle+distance/cumulative_prop.png "title")

![alt-text-1](/figures/advanced models/xgb-angle+distance/reliability.png "title")

Comparing the results with the logistic regression baseline we can see, Distance Feature can bring a little bit more sense to predict goals than angle feature.

Compared to Logistic Regression, 
As we can see in the ROC plot, performance is improved, Goal rate is almost similar for both algorithms, similarly Cumulative Goal plot area under the curve for xgboost is improved a bit and lastly reliability for xgboost using both features is also higher compared to logistic regression. 


Q2:
[Comet_entry](https://www.comet.com/nhl-07/advanced-models-2/5116b5b08e32432b93c44d07f350e378?experiment-tab=chart&showOutliers=true&smoothing=0&transformY=smoothing&xAxis=step)

Training xgboost classifier with all the features from feature engineering 2

Features Used: 
['game_time', 'period', 'xcoord', 'ycoord', 'dist_net', 'angle_net', 'shot_type', 'prev_event_type', 'prev_xcoord', 'prev_ycoord', 'time_from_prev', 'prev_dist', 'rebound', 'change_in_angle', 'speed']

If we recall the requirements for xgboost model, we have to scale the data and encode the categorical features before training the model,

For scaling we will use standardization with the help of sklearn’s StandardScalar. 
Here we also have categorical features as following:
 ['period', 'shot_type', 'prev_event_type', 'rebound']

Also since this is a problem with imbalance classification, we have set class weights according to their frequency using compute_sample_weight from sklearn.

We tried several approaches (OneHotEncoder, LabelEncoder, Dummies and frequency encoding) using different categorical encoders and found that using frequency encoding we can better encode (in low dimensions) the features without much effect on results. We have used CountEncoder from the category_encoders library.

Hyperparameter Tuning:

For tuning the hyperparameters we tried 3 approaches (experiments can be found in obsolete_code folder in the repository)
* Grid Search (Exhaustive)
* Randomized Search
* Bayesian Search

We found Bayesian Search of parameters to be consistent and very much closer to Exhaustive grid search which needs very much computing as it goes through every possible combination of the parameters list in the defined searching grid.

BayesianSearchCV:
In contrast to GridSearchCV, not all parameter values are tried out, but rather a fixed number of parameter settings is sampled from the specified distributions.

For strategy, it is not worthy to use accuracy as there is an imbalance in labels, so we are going with ROC AUC.

The following image shows log of bayesian search execution

![alt-text-1](/figures/advanced models/fig3.png "Title")


Comparison with XGBoost Baseline:

![alt-text-1](/figures/advanced models/xgb-best-hptune/roc.png "title")

![alt-text-1](/figures/advanced models/xgb-best-hptune/goal_rate.png "title")

![alt-text-1](/figures/advanced models/xgb-best-hptune/cumulative_prop.png "title")

![alt-text-1](/figures/advanced models/xgb-best-hptune/reliability.png "title")

Compared to baseline this models perform good, accuracy is increased but still overall performance of this model is also not upto the mark, as expected because of the imbalance in the class labels.

Comparing the plots ROC plot also reflects improvement as there is more area under the curve, similarly goal rate and cumulative percentile plots also suggest improvement. Reliability curve however reflects that the model becomes very much unreliable as the line for the model should be stuck to the support line(black line) - which suggests an ideal classifier.


Q3: 
[Comet_entry](https://www.comet.com/nhl-07/advanced-models-3/aaecb664926b4957812054ef6d982376?experiment-tab=chart&showOutliers=true&smoothing=0&transformY=smoothing&xAxis=step)

As a part of this question we have to select quality features, for that we will use best model that we found by using hyperparameter tuning and using class weights,

We will train the model using the best parameters that we found in Q2.

For Feature selection we will first look into trained model’s feature importance, fortunately, xgboost has a method (plot_importance()) to plot the importance of features.

If we look at this we can see that 

![alt-text-1](/figures/advanced models/fig4.png "Title")

But this values only represent weight/frequency, using this [article](https://towardsdatascience.com/be-careful-when-interpreting-your-features-importance-in-xgboost-6e16132588e7) we found that using only this plot to decide which features to include can be a mistake.

Using Gain (Information Gain) to reflect feature importance:

![alt-text-1](/figures/advanced models/fig5.png "Title")

Approach 2:
Using scikit-learn’s feature selection. Scikit-learn library has some very good feature selection techniques implemented like SelectKBest, ForwardSearch, BackWordSearch etc,
Since we are not sure and bound to how many features we can use, we  
We tried SelectFromModel() 

Approach 3:

What is SHAP?
SHAP (SHapley Additive exPlanations) is a game theoretic approach to explain the output of any machine learning model. It connects optimal credit allocation with local explanations using the classic Shapley values from game theory and their related extensions.

Absolute SHAP value horizontal bar plot:

![alt-text-1](/figures/advanced models/fig6.png "Title")

SHAP summary plot:

![alt-text-1](/figures/advanced models/fig7.png "Title")

After assessing results from these experiments we found a smaller set of features that can be used to classify shots.

Selected Features

selected_features = ['game_time', 'ycoord', 'dist_net', 'angle_net', 'time_from_prev',    'change_in_angle', 'shot_type_count', 'prev_event_type_count']

A new model was trained on these selected features, using class weights and best parameters from Q2,

Comparing results with XGBoost baseline.

![alt-text-1](/figures/advanced models/xgb-final/roc.png "title")

![alt-text-1](/figures/advanced models/xgb-final/goal_rate.png "title")

![alt-text-1](/figures/advanced models/xgb-final/cumulative_prop.png "title")

![alt-text-1](/figures/advanced models/xgb-final/reliability.png "title")


As compared to baseline models, the massive improvements can be seen, 
Accuracy was decreased, but as the goal of these experiments is to try to predict quality shots improvement on recall and True Positives was increased.

ROC plot depicts the area under the curve is increased compared to the xgboost baseline model. Goal rate is also improved compared to baseline, Similarly, Reliability curve and Cumulative percentile lines show improvement in performance. However the reliability line for this model is further away from the ideal classifier than baseline model’s. This shows the model's reliability is not good.



## Step 6: Give it your best shot!

i.
Final Best model comet entry: [here](https://www.comet.com/nhl-07/try-best/f34f70ae3bcb41cab27ab4461a108426?experiment-tab=chart&showOutliers=true&smoothing=0&transformY=smoothing&xAxis=step).
We approached this task wanting get a better idea at how to handle unbalanced data. The goal to shot ratio is about 1:19, thus our data set was quite unbalanced. We can see clearly in the previous couple tasks that our models, and specifically they way we trained our models, yielded pretty much useless results as all X_i points were getting classified as non-goals. From IMBLearn, there is a function called SMOTE which uses the principles from K-Nearest Neighbours to synthetically generate more statistically similar training data-points from the minority class. We decided to use this approach along with better feature selection methods to optimize a simple Neural Network and an XGBoost model which is best suited for this type of problem.

Our first model was just a baseline XGBoost model with all relevant/possible features [here](https://www.comet.com/nhl-07/try-best/67ec3c39233a491f8caa17d9ad304ddd). The second model was an XGBoost using a feature selection method that selected the best set of features for performance ranked on the feature's importance coefficients [here](https://www.comet.com/nhl-07/try-best/369a7d23e3124c6f91dc9b25246f390f). Our third model was an XGBoost using the SMOTE to generate more goal data points [here](https://www.comet.com/nhl-07/try-best/31ad70d81a024e30bebbf7c6532f68d4). Our fourth also used XGBoost but used the feature selection and SMOTE approach combined [here](https://www.comet.com/nhl-07/try-best/8ff12dd37c8c482680d2afdc98996c43). We then moved on to neural networks. The fifth model was just a 3-layer network [here](https://www.comet.com/nhl-07/try-best/e93c967b981c4be289fbb39f3b0d71e5) and the sixth was the same network with the SMOTE approach [here](https://www.comet.com/nhl-07/try-best/125ec88b646b426bbb257b34acf54a31).

We also wanted to focus on different metrics. For unbalanced datasets, we need to use metrics such as f1-score, sensitivity, specificity, positive predictive value and negative preductive value. These metrics will help use determine which classes are being predicted accurately. 

### Metrics

| Model     | Sensitivity | Specificity      | F1-Score | PPV     | NPV |
| ----------- | ----------- | ----------- | ----------- | ----------- | ----------- |
| XGB Base    | 0.15 | 0.99    | 0.60 | 0.69     | 0.92 |
| XBG + Freature Selection     | 0.06 | 1.00     | 0.54 | 0.82     | 0.91 |
| XBG + SMOTE  | 0.15| 0.99      | 0.60 | 0.68     | 0.92 |
|  XBG + SMOTE +FS  | 0.05 | 1.00     | 0.52| 0.64     | 0.91 |
| NN Base    | 0.56| 0.20     | 0.22 | 0.07    | 0.82 |
| NN +SMOTE   | 0.34 | 0.34    | 0.28 | 0.05     | 0.83 |



![alt-text-1](/figures/realiability.png "title-19") ![alt-text-2](/figures/roc.png "title-21")
![alt-text-1](/figures/cumalitive.png "title-12") ![alt-text-2](/figures/goal_rate.png "title-23")

What we got from testing these models is that the binary classification is extremely difficult. We're faced with the extreme tradeoff of PPV ad sensitivity. That is to say, we can classify many more goals correctly but at the extreme expense of precision and vice-versa. After analysis of results, we determined that XGB: SMOTE is the best predictor with a relatively high f1-score. 



## Step 7: Evaluate on test set


Try your best:

Regular Season:

| Model        | Sensitivity | Specificity | F1-Score    | PPV     | NPV |
| -----------  | ----------- | ----------- | ----------- | --------| ----------- |
| XGB SMOTE    | 0.19        | 0.97        | 0.60        | 0.40    | 0.92 |

![alt-text-1](/figures/advanced models/model-comparison/roc.png "title")

![alt-text-1](/figures/advanced models/model-comparison/goal_rate.png "title")

![alt-text-1](/figures/advanced models/model-comparison/percentile.png "title")

![alt-text-1](/figures/advanced models/model-comparison/calibration.png "title")


Looking at the results and plots, we can see that models are performing as expected, the order of model’s performance is somewhat consistent and as expected, xgboost with smote resulted with highest performance. (here performance is referred as classifying good shots that can be resulted in goal)

Looking at the ROC curve we can see that XGBoost model from try your best shot used with the smote imbalance handling technique gets the highest area under the curve, following that, xgboost model from advance modeling used with class weight, hyper-parameter tuning and feature selection performs well. Here surprisingly neural networks with all features perform equally(even somewhat poorly) to logistic regression with just distance features. Generally neural networks are known as a set of machine learning algorithms that can detect underlying patterns in data well. 
Logistic regression with both features performs identical to logistic regression with only distance feature. Logistic regression with angle features performs worst like a random baseline.

Models ordered by their performance:
* Xgboost with smote
* Xgboost with hyperparameter tuning, feature selection and class weights
* Logistic regression with Distance Feature
* Logistic regression with distance and angle feature
* Neural network with smote
* Logistic regression with angle feature



Playoff Season

| Model       | Sensitivity | Specificity | F1-Score    | PPV         | NPV |
| ----------- | ----------- | ----------- | ----------- | ----------- | ----------- |
| XGB SMOTE   | 0.17        | 0.99        | 0.61        | 0.46        | 0.92 |


![alt-text-1](/figures/advanced models/model-comparison/roc-playoff.png "title")

![alt-text-1](/figures/advanced models/model-comparison/goal_rate-playoff.png "title")

![alt-text-1](/figures/advanced models/model-comparison/percentile-playoff.png "title")

![alt-text-1](/figures/advanced models/model-comparison/calibration-playoff.png "title")

Similar to test set on regular seasons, models perform on playoff test set. XGBoost with smote dominates all other classifiers, followed by hyper parameters tuned xgboost.
We have found that well tuned xgboost generalizes well compared to other algorithms. It was somewhat expected from the reliability plot difference in neural network and xgboost.

Models ordered by their performance:
* Xgboost with smote
* Xgboost with hyperparameter tuning, feature selection and class weights
* Logistic regression with Distance Feature
* Logistic regression with distance and angle feature
* Neural network with smote
* Logistic regression with angle feature





